{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-learning",
   "metadata": {},
   "source": [
    "# LAB 3: Automated Terminology Extraction\n",
    "\n",
    "Extract technical terms from ACL Anthology\n",
    "\n",
    "Objectives:\n",
    "* part of speech tagging with spacy\n",
    "* extract phrases that match a part of speech pattern\n",
    "* scale processing pipeline with dask\n",
    "* compute c-values\n",
    "\n",
    "## Part I: Test c-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "later-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serious-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/acl.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1001</td>\n",
       "      <td>Polynomial Time Parsing of Combinatory Categor...</td>\n",
       "      <td>Polynomial Time Parsing of Combinatory Categor...</td>\n",
       "      <td>In this paper we present a polynomial time par...</td>\n",
       "      <td>Combinatory Categorial Grammar (CCG)  is an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1002</td>\n",
       "      <td>Structure and Intonation in Spoken Language Un...</td>\n",
       "      <td>Structure and Intonation in Spoken Language Un...</td>\n",
       "      <td>The structure imposed upon spoken sentences by...</td>\n",
       "      <td>Halliday  observed that this constraint, which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1003</td>\n",
       "      <td>Prosody, Syntax and Parsing</td>\n",
       "      <td>Prosody, Syntax and Parsing We describe the mo...</td>\n",
       "      <td>We describe the modification of a grammar to t...</td>\n",
       "      <td>Prosodic information can mark lexical stress, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1004</td>\n",
       "      <td>Empirical Study of Predictive Powers of Simple...</td>\n",
       "      <td>Empirical Study of Predictive Powers of Simple...</td>\n",
       "      <td>This empirical study attempts to find answers ...</td>\n",
       "      <td>Difficulty in resolving structural ambiguity i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>P90-1005</td>\n",
       "      <td>Structural Disambiguation With Constraint Prop...</td>\n",
       "      <td>Structural Disambiguation With Constraint Prop...</td>\n",
       "      <td>We present a new grammatical formalism called ...</td>\n",
       "      <td>We are interested in an efficient treatment of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       tag                                              title  \\\n",
       "0  1990  P90-1001  Polynomial Time Parsing of Combinatory Categor...   \n",
       "1  1990  P90-1002  Structure and Intonation in Spoken Language Un...   \n",
       "2  1990  P90-1003                        Prosody, Syntax and Parsing   \n",
       "3  1990  P90-1004  Empirical Study of Predictive Powers of Simple...   \n",
       "4  1990  P90-1005  Structural Disambiguation With Constraint Prop...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Polynomial Time Parsing of Combinatory Categor...   \n",
       "1  Structure and Intonation in Spoken Language Un...   \n",
       "2  Prosody, Syntax and Parsing We describe the mo...   \n",
       "3  Empirical Study of Predictive Powers of Simple...   \n",
       "4  Structural Disambiguation With Constraint Prop...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper we present a polynomial time par...   \n",
       "1  The structure imposed upon spoken sentences by...   \n",
       "2  We describe the modification of a grammar to t...   \n",
       "3  This empirical study attempts to find answers ...   \n",
       "4  We present a new grammatical formalism called ...   \n",
       "\n",
       "                                                body  \n",
       "0  Combinatory Categorial Grammar (CCG)  is an ex...  \n",
       "1  Halliday  observed that this constraint, which...  \n",
       "2  Prosodic information can mark lexical stress, ...  \n",
       "3  Difficulty in resolving structural ambiguity i...  \n",
       "4  We are interested in an efficient treatment of...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "canadian-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6167"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-deployment",
   "metadata": {},
   "source": [
    "We will use random sample (500) of the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "essential-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(500, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-ferry",
   "metadata": {},
   "source": [
    "### spaCy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "primary-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-temple",
   "metadata": {},
   "source": [
    "Loading a processing pipeline.  This is a small English model.  Will be using part of speech labels only, so we will be excluding modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "front-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner', 'lemmatizer', 'attribute_ruler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "silver-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f312b50d4a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f312b4bdf90>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suffering-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collective-definition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resume Information Extraction with Cascaded Hybrid Model This paper presents an effective approach for resume information extraction to support automatic resume management and routing. A cascaded information extraction (IE) framework is designed. In the first pass, a resume is segmented into a consecutive blocks attached with labels indicating the information types. Then in the second pass, the detailed information, such as Name and Address, are identified in certain blocks (e.g. blocks labelled with Personal Information), instead of searching globally in the entire resume. The most appropriate model is selected through experiments for each IE task in different passes. The experimental results show that this cascaded hybrid model achieves better F-score than flat models that do not apply the hierarchical structure of resumes. It also shows that applying different IE models in different passes according to the contextual structure is effective. Big enterprises and head-hunters receive hundreds of resumes from job applicants every day. Automatically extracting structured information from resumes of different styles and formats is needed to support the automatic construction of database, searching and resume routing. The definition of resume information fields varies in different applications. Normally, resume information is described as a hierarchical structure . Predefined information types. Based on the requirements of an ongoing recruitment management system which incorporates database construction with IE technologies and resume recommendation (routing), as shown in , 7 general information fields are defined. Then, for Personal Information, 14 detailed information fields are designed; for Education, 4 detailed information fields are designed. The IE task, as exemplified in , includes segmenting a resume into consecutive blocks labelled with general information types, and further extracting the detailed information such as Name and Address from certain blocks. Extracting information from resumes with high precision and recall is not an easy task. In spite of constituting a restricted domain, resumes can be written in multitude of formats (e.g. structured tables or plain texts), in different languages (e.g. Chinese and English) and in different file types (e.g. Text, PDF, Word etc.). Moreover, writing styles could be very diversified. Among the methods in IE, Hidden Markov modelling has been widely used . As a statebased model, HMMs are good at extracting information fields that hold a strong order of sequence. Classification is another popular method in IE. By assuming the independence of information types, it is feasible to classify segmented units as either information types to be extracted , or information boundaries . This method specializes in settling the extraction problem of independent information types. Resume shares a document-level hierarchical contextual structure where the related information units usually occur in the same textual block, and text blocks of different information categories usually occur in a relatively fixed order. Such characteristics have been successfully used in the categorization of multi-page documents by . In this paper, given the hierarchy of resume information, a cascaded two-pass IE framework is designed. In the first pass, the general information is extracted by segmenting the entire resume into consecutive blocks and each block is annotated with a label indicating its category. In the second pass, detailed information pieces are further extracted within the boundary of certain blocks. Moreover, for different types of information, the most appropriate extraction method is selected through experiments. For the first pass, since there exists a strong sequence among blocks, a HMM model is applied to segment a resume and each block is labelled with a category of general information. We also apply HMM for the educational detailed information extraction for the same reason. In addition, classification based method is selected for the personal detailed information extraction where information items appear relatively independently. Tested with 1,200 Chinese resumes, experimental results show that exploring the hierarchical structure of resumes with this proposed cascaded framework improves the average F-score of detailed information extraction greatly, and combining different IE models in different layer properly is effective to achieve good precision and recall. The remaining part of this paper is structured as follows. Section 2 introduces the related work. Section 3 presents the structure of the cascaded hybrid IE model and introduces the HMM model and SVM model in detail. Experimental results and analysis are shown in Section 4. Section 5 provides a discussion of our cascaded hybrid model. Section 6 is the conclusion and future work. As far as we know, there are few published works on resume IE except some products, for which there is no way to determine the technical details. One of the published results on resume IE was shown in . In this work, they applied (LP) 2 , a toolkit of IE, to learn information extraction rules for resumes written in English. The information defined in their task includes a flat structure of Name, Street, City, Province, Email, Telephone, Fax and Zip code. This flat setting is not only different from our hierarchical structure but also different from our detailed information pieces. Besides, there are some applications that are analogous to resume IE, such as seminar announcement IE , job posting IE  and address segmentation ). Most of the approaches employed in these applications view a text as flat and extract information from all the texts directly . Only a few approaches extract information hierarchically like our model.  present a double classification approach to perform IE by extracting words from pre-extracted sentences.  develop a nested model, where the outer HMM captures the sequencing relationship among elements and the inner HMMs learn the finer structure within each element. But these approaches employ the same IE methods for all the information types. Compared with them, our model applies different methods in different sub-tasks to fit the special contextual structure of information in each sub-task well.  is the structure of our cascaded hybrid model. The first pass (on the left hand side) segments a resume into consecutive blocks with a HMM model. Then based on the result, the second pass (on the right hand side) uses HMM to extract the educational detailed information and SVM to extract the personal detailed information, respectively. The block selection module is used to decide the range of detailed information extraction in the second pass.  For general information, the IE task is viewed as labelling the segmented units with predefined class labels. Given an input resume T which is a sequence of words w 1 ,w 2 ,…,w k , the result of general information extraction is a sequence of blocks in which some words are grouped into a certain block T = t 1 , t 2 ,…, t n , where t i is a block. Assuming the expected label sequence of T is L=l 1 , l 2 ,…, l n , with each block being assigned a label l i , we get the sequence of block and label pairs Q=(t 1 , l 1 ), (t 2 , l 2 ),…,(t n , l n ). In our research, we simply assume that the segmentation is based on the natural paragraph of T.  gives the list of information types to be extracted, where general information is represented as G 1~G7 . For each kind of general information, say G i , two labels are set: G i -B means the beginning of G i , G i -M means the remainder part of G i . In addition, label O is defined to represent a block that does not belong to any general information types. With these positional information labels, general information can be obtained. For instance, if the label sequence Q for a resume with 10 paragraphs is , three types of general information can be extracted as follows: Formally, given a resume T=t 1 ,t 2 ,…,t n , seek a label sequence L * =l 1 ,l 2 ,…,l n , such that the probability of the sequence of labels is maximal. (1) According to Bayes' equation, we have ) ( (2) If we assume the independent occurrence of blocks labelled as the same information types, we have We assume the independence of words occurring in t i and use a unigram model, which multiplies the probabilities of these words to get the probability of If a tri-gram model is used to estimate P(L), we have To extract educational detailed information from Education general information, we use another HMM. It also uses two labels D i -B and D i -M to represent the beginning and remaining part of D i , respectively. In addition, we use label O to represent that the corresponding word does not belong to any kind of educational detailed information. But this model expresses a text T as word sequence T=w 1 ,w 2 ,…,w n . Thus in this model, the probability P(L) is calculated with Formula 5 and the probability P(T|L) is calculated by Here we assume the independent occurrence of words labelled as the same information types. Both words and named entities are used as features in our HMMs. A Chinese resume C= c 1 ',c 2 ',…,c k ' is first tokenized into C= w 1 ,w 2 ,…,w k with a Chinese word segmentation system LSP . This system outputs predefined features, including words and named entities in 8 types (Name, Date, Location, Organization, Phone, Number, Period, and Email). The named entities of the same type are normalized into single ID in feature set. In both HMMs, fully connected structure with one state representing one information label is applied due to its convenience. To estimate the probabilities introduced in 3.1.1, maximum likelihood estimation is used, which are Short of training data to estimate probability is a big problem for HMMs. Such problems may occur when estimating either P(T|L) with unknown word w i or P(L) with unknown events.  mapped all unknown words to one token _UNK_ and then used a held-out data to train the bi-gram models where unknown words occur. They also applied a back-off strategy to solve the data sparseness problem when estimating the context model with unknown events, which interpolates the estimation from training corpus and the estimation from the back-off model with calculated parameter λ .  used shrinkage to estimate the emission probability of unknown words, which combines the estimates from data-sparse states of the complex model and the estimates in related data-rich states of the simpler models with a weighted average. In our HMMs, we first apply Good Turing smoothing  to estimate the probability P(w r |l i ) when training data is sparse. For word w r seen in training data, the emission probability is P(w r |l i )×(1-x), where P(w r |l i ) is the emission probability calculated with Formula 9 and x=E i /S i (E i is the number of words appearing only once in state i and S i is the total number of words occurring in state i). For unknown word w r , the emission probability is x/(M-m i ), where M is the number of all the words appearing in training data, and m i is the number of distinct words occurring in state i. Then, we use a back-off schema  to deal with the data sparseness problem when estimating the probability P(L) . We convert personal detailed information extraction into a classification problem. Here we select SVM as the classification model because of its robustness to over-fitting and high performance . In the SVM model, the IE task is also defined as labelling segmented units with predefined class labels. We still use two labels to represent personal detailed information P i : P i -B represents the beginning of P i and P i -M represents the remainder part of P i . Besides of that, label O means that the corresponding unit does not belong to any personal detailed information boundaries and information types. For example, for part of a resume \"Name:Alice (Female)\", we got three units after segmentation with punctuations, i.e. \"Name\", \"Alice\", \"Female\". After applying SVM classification, we can get the label sequence as P 1 -B,P 1 -M,P 2 -B. With this sequence of unit and label pairs, two types of personal detailed information can be extracted as P 1 : [Name:Alice] and P 2 : Various ways can be applied to segment T. In our work, segmentation is based on the natural sentence of T. This is based on the empirical observation that detailed information is usually separated by punctuations (e.g. comma, Tab tag or Enter tag). The extraction of personal detailed information can be formally expressed as follows: given a text T=t 1 ,t 2 ,…,t n , where t i is a unit defined by the segmenting method mentioned above, seek a label sequence L* = l 1 ,l 2 ,…,l n , such that the probability of the sequence of labels is maximal. The key assumption to apply classification in IE is the independence of label assignment between units. With this assumption, Formula 10 can be described as Thus this probability can be maximized by maximizing each term in turn. Here, we use the SVM score of labelling t i with l i to replace P(l i |t i ). SVM is a binary classification model. But in our IE task, it needs to classify units into N classes, where N is two times of the number of personal detailed information types. There are two popular strategies to extend a binary classification task to N classes (A. . The first is One vs. All strategy, where N classifiers are built to separate one class from others. The other is Pairwise strategy, where N×(N-1)/2 classifiers considering all pairs of classes are built and final decision is given by their weighted voting. In our model, we apply the One vs. All strategy for its good efficiency in classification. We construct one classifier for each type, and classify each unit with all these classifiers. Then we select the type that has the highest score in classification. If the selected score is higher than a predefined threshold, then the unit is labelled as this type. Otherwise it is labelled as O. Features defined in our SVM model are described as follows: Word: Words that occur in the unit. Each word appearing in the dictionary is a feature. We use TF×IDF as feature weight, where TF means word frequency in the text, and IDF is defined as:  Block selection is used to select the blocks generated from the first pass as the input of the second pass for detailed information extraction. Error analysis of preliminary experiments shows that the majority of the mistakes of general information extraction resulted from labelling non-  . IE results with cascaded model and flat model. boundary blocks as boundaries in the first pass. Therefore we apply a fuzzy block selection strategy, which not only selects the blocks labelled with target general information, but also selects their neighboring two blocks, so as to enlarge the extracting range. We evaluated this cascaded hybrid model with 1,200 Chinese resumes. The data set was divided into 3 parts: training data, parameter tuning data and testing data with the proportion of 4:1:1. 6folder cross validation was conducted in all the experiments. We selected SVMlight  as the SVM classifier toolkit and LSP  for Chinese word segmentation and named entity identification. Precision (P), recall (R) and F-score (F=2PR/(P+R)) were used as the basic evaluation metrics and macro-averaging strategy was used to calculate the average results. For the special application background of our resume IE model, the \"Overlap\" criterion  was used to match reference instances and extracted instances. We define that if the proportion of the overlapping part of extracted instance and reference instance is over 90%, then they match each other. A set of experiments have been designed to verify the effectiveness of exploring documentlevel hierarchical structure of resume and choose the best IE models (HMM vs. classification) for each sub-task. Cascaded model vs. flat model Two flat models with different IE methods (SVM and HMM) are designed to extract personal detailed information and educational detailed information respectively. In these models, no hierarchical structure is used and the detailed information is extracted from the entire resume texts rather than from specific blocks. These two flat models will be compared with our proposed cascaded model. Both SVM and HMM are tested for all the IE tasks in first pass and in second pass. We tested the flat model and cascaded model with detailed information extraction to verify the effectiveness of exploring document-level hierarchical structure. Results (see ) show that with the cascaded model, the precision is greatly improved compared with the flat model with identical IE method, especially for educational detailed information. Although there is some loss in recall, the average F-score is still largely improved in the cascaded model. Then we tested different models for the general information and detailed information to choose the most appropriate IE model for each sub-task.   . Detailed information extraction with different models. Results (see ) show that compared with SVM, HMM achieves better recall. In our cascaded framework, the extraction range of detailed information is influenced by the result of general information extraction. Thus better recall of general information leads to better recall of detailed information subsequently. For this reason, we choose HMM in the first pass of our cascaded hybrid model. Then in the second pass, different IE models are tested in order to select the most appropriate one for different sub-tasks. Results (see ) show that HMM performs much better in both precision and recall than SVM for educational detailed information extraction. We think that this is reasonable because HMM takes into account the sequence constraints among educational detailed information types. Therefore HMM model is selected to extract educational detailed information in our cascaded hybrid model. While for the personal detailed information extraction, we find that the SVM model gets better precision and recall than HMM model. We think that this is because of the independent occurrence of personal detailed information. Therefore, we select SVM to extract personal detailed information in our cascaded model. Our cascaded framework is a \"pipeline\" approach and it may suffer from error propagation. For instance, the error in the first pass may be transferred to the second pass when determining the extraction range of detailed information. Therefore the precision and recall of detailed information extraction in the second pass may be decreased subsequently. But we are not sure whether N-Best approach  would be helpful. Because our cascaded hybrid model applies different IE methods for different sub-tasks, it is difficult to incorporate the N-best strategy by either simply combining the scores of the first pass and the second pass, or using the scores of the second pass to do re-ranking to select the best results. Instead of using N-best, we apply a fuzzy block selection strategy to enlarge the search scope. Experimental results of personal detailed information extraction show that compared with the exact block selection strategy, this fuzzy strategy improves the average recall of personal detailed information from 68.48% to 71.34% and reduce the average precision from 83.27% to 81.71%. Therefore the average F-score is improved by the fuzzy strategy from 75.15% to 76.17%. Features are crucial to our SVM model. For some fields (such as Name, Address and Graduation School), only using words as features may result in low accuracy in IE. The named entity (NE) features used in our model enhance the accuracy of detailed information extraction. As exemplified by the results (see ) on personal detailed information extraction, after adding named entity features, the F-score are improved greatly.  . Personal detailed information extraction with different features (Avg.F). In our cascaded hybrid model, we apply HMM and SVM in different pass separately to explore the contextual structure of information types. It guarantees the simplicity of our hybrid model. However, there are other ways to combine statebased and discriminative ideas. For example,  applied Conditional Random Fields to extract information, which draws together the advantages of both HMM and SVM. This approach could be considered in our future experiments. Some personal detailed information types do not achieve good average F-score in our model, such as Zip code (74.50%) and Mobile (73.90%). Error analysis shows that it is because these fields do not contain distinguishing words and named entities. For example, it is difficult to extract Mobile from the text \"Phone: 010-62617711 (13859750123)\". But these fields can be easily distinguished with their internal characteristics. For example, Mobile often consists of certain length of digital figures. To identify these fields, the Finite-State Automaton (FSA) that employs hand-crafted grammars is very effective . Alternatively, rules learned from annotated data are also very promising in handling this case . We assume the independence of words occurring in unit t i to calculate the probability P(t i |l i ) in HMM model. While in , a bi-gram model is applied where each word is conditioned on its immediate predecessor when generating words inside the current name-class. We will compare this method with our current method in the future. We have shown that a cascaded hybrid model yields good results for the task of information extraction from resumes. We tested different models for the first pass and the second pass, and for different IE tasks. Our experimental results show that the HMM model is effective in handling the general information extraction and educational detailed information extraction, where there exists strong sequence of information pieces. And the SVM model is effective for the personal detailed information extraction. We hope to continue this work in the future by investigating the use of other well researched IE methods. As our future works, we will apply FSA or learned rules to improve the precision and recall of some personal detailed information (such as Zip code and Mobile). Other smoothing methods such as  will be tested in order to better overcome the data sparseness problem."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surprised-fellowship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resume Information Extraction with Cascaded Hybrid Model This paper presents"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alike-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disturbed-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DT', 'the')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[200].tag_, doc[200].norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-renewal",
   "metadata": {},
   "source": [
    "We will import the rule-based matcher from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "municipal-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compatible-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN']}},  #JJ = adjective  #NN = noun\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN', 'HYPH']}, 'OP': '*'},  ##IN = preposition ##HYPH = hyphenated speech\n",
    "                      {'TAG': 'NN'}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "noted-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = matcher(doc, as_spans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-organ",
   "metadata": {},
   "source": [
    "This is the first candidate in the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "appropriate-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('effective', 'approach')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(tok.norm_ for tok in spans[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-seeking",
   "metadata": {},
   "source": [
    "### Extract candidate terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dirty-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text):\n",
    "    doc = nlp(text)  #tokenize and tag\n",
    "    spans = matcher(doc, as_spans=True)  #find all the tags\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans] #return a list of all spans converted into tuples of normalized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "coordinate-quarter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('effective', 'approach'),\n",
       " ('approach', 'for', 'resume'),\n",
       " ('effective', 'approach', 'for', 'resume'),\n",
       " ('resume', 'information'),\n",
       " ('approach', 'for', 'resume', 'information'),\n",
       " ('effective', 'approach', 'for', 'resume', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('resume', 'information', 'extraction'),\n",
       " ('approach', 'for', 'resume', 'information', 'extraction'),\n",
       " ('effective', 'approach', 'for', 'resume', 'information', 'extraction'),\n",
       " ('automatic', 'resume'),\n",
       " ('resume', 'management'),\n",
       " ('automatic', 'resume', 'management'),\n",
       " ('cascaded', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('cascaded', 'information', 'extraction'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('detailed', 'information'),\n",
       " ('entire', 'resume'),\n",
       " ('appropriate', 'model'),\n",
       " ('hybrid', 'model'),\n",
       " ('f', '-', 'score'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('contextual', 'structure'),\n",
       " ('structured', 'information'),\n",
       " ('automatic', 'construction'),\n",
       " ('construction', 'of', 'database'),\n",
       " ('automatic', 'construction', 'of', 'database'),\n",
       " ('definition', 'of', 'resume'),\n",
       " ('resume', 'information'),\n",
       " ('definition', 'of', 'resume', 'information'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('ongoing', 'recruitment'),\n",
       " ('recruitment', 'management'),\n",
       " ('ongoing', 'recruitment', 'management'),\n",
       " ('management', 'system'),\n",
       " ('recruitment', 'management', 'system'),\n",
       " ('ongoing', 'recruitment', 'management', 'system'),\n",
       " ('database', 'construction'),\n",
       " ('general', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('general', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('high', 'precision'),\n",
       " ('easy', 'task'),\n",
       " ('restricted', 'domain'),\n",
       " ('different', 'file'),\n",
       " ('statebased', 'model'),\n",
       " ('strong', 'order'),\n",
       " ('order', 'of', 'sequence'),\n",
       " ('strong', 'order', 'of', 'sequence'),\n",
       " ('popular', 'method'),\n",
       " ('independence', 'of', 'information'),\n",
       " ('extraction', 'problem'),\n",
       " ('independent', 'information'),\n",
       " ('problem', 'of', 'independent', 'information'),\n",
       " ('extraction', 'problem', 'of', 'independent', 'information'),\n",
       " ('document', '-', 'level'),\n",
       " ('contextual', 'structure'),\n",
       " ('hierarchical', 'contextual', 'structure'),\n",
       " ('level', 'hierarchical', 'contextual', 'structure'),\n",
       " ('document', '-', 'level', 'hierarchical', 'contextual', 'structure'),\n",
       " ('related', 'information'),\n",
       " ('textual', 'block'),\n",
       " ('same', 'textual', 'block'),\n",
       " ('different', 'information'),\n",
       " ('hierarchy', 'of', 'resume'),\n",
       " ('resume', 'information'),\n",
       " ('hierarchy', 'of', 'resume', 'information'),\n",
       " ('first', 'pass'),\n",
       " ('general', 'information'),\n",
       " ('entire', 'resume'),\n",
       " ('second', 'pass'),\n",
       " ('detailed', 'information'),\n",
       " ('appropriate', 'extraction'),\n",
       " ('extraction', 'method'),\n",
       " ('appropriate', 'extraction', 'method'),\n",
       " ('first', 'pass'),\n",
       " ('strong', 'sequence'),\n",
       " ('general', 'information'),\n",
       " ('category', 'of', 'general', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('educational', 'detailed', 'information', 'extraction'),\n",
       " ('same', 'reason'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('average', 'f'),\n",
       " ('f', '-', 'score'),\n",
       " ('average', 'f', '-', 'score'),\n",
       " ('detailed', 'information'),\n",
       " ('score', 'of', 'detailed', 'information'),\n",
       " ('f', '-', 'score', 'of', 'detailed', 'information'),\n",
       " ('average', 'f', '-', 'score', 'of', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('score', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('f', '-', 'score', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('average', 'f', '-', 'score', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('different', 'layer'),\n",
       " ('good', 'precision'),\n",
       " ('related', 'work'),\n",
       " ('model', 'in', 'detail'),\n",
       " ('hybrid', 'model'),\n",
       " ('future', 'work'),\n",
       " ('information', 'extraction'),\n",
       " ('flat', 'structure'),\n",
       " ('flat', 'setting'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('detailed', 'information'),\n",
       " ('such', 'as', 'seminar'),\n",
       " ('seminar', 'announcement'),\n",
       " ('such', 'as', 'seminar', 'announcement'),\n",
       " ('double', 'classification'),\n",
       " ('classification', 'approach'),\n",
       " ('double', 'classification', 'approach'),\n",
       " ('different', 'sub'),\n",
       " ('contextual', 'structure'),\n",
       " ('special', 'contextual', 'structure'),\n",
       " ('structure', 'of', 'information'),\n",
       " ('contextual', 'structure', 'of', 'information'),\n",
       " ('special', 'contextual', 'structure', 'of', 'information'),\n",
       " ('sub', '-'),\n",
       " ('-', 'task'),\n",
       " ('sub', '-', 'task'),\n",
       " ('hybrid', 'model'),\n",
       " ('first', 'pass'),\n",
       " ('left', 'hand'),\n",
       " ('hand', 'side'),\n",
       " ('left', 'hand', 'side'),\n",
       " ('hmm', 'model'),\n",
       " ('second', 'pass'),\n",
       " ('right', 'hand'),\n",
       " ('hand', 'side'),\n",
       " ('right', 'hand', 'side'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('block', 'selection'),\n",
       " ('selection', 'module'),\n",
       " ('block', 'selection', 'module'),\n",
       " ('detailed', 'information'),\n",
       " ('range', 'of', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('range', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('second', 'pass'),\n",
       " ('general', 'information'),\n",
       " ('predefined', 'class'),\n",
       " ('input', 'resume'),\n",
       " ('general', 'information'),\n",
       " ('result', 'of', 'general', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('general', 'information', 'extraction'),\n",
       " ('result', 'of', 'general', 'information', 'extraction'),\n",
       " ('certain', 'block'),\n",
       " ('block', 't'),\n",
       " ('certain', 'block', 't'),\n",
       " ('label', 'sequence'),\n",
       " ('sequence', 'of', 't'),\n",
       " ('label', 'sequence', 'of', 't'),\n",
       " ('label', 'l'),\n",
       " ('sequence', 'of', 'block'),\n",
       " ('natural', 'paragraph'),\n",
       " ('list', 'of', 'information'),\n",
       " ('general', 'information'),\n",
       " ('general', 'information'),\n",
       " ('kind', 'of', 'general', 'information'),\n",
       " ('part', 'of', 'g'),\n",
       " ('label', 'o'),\n",
       " ('general', 'information'),\n",
       " ('positional', 'information'),\n",
       " ('general', 'information'),\n",
       " ('label', 'sequence'),\n",
       " ('general', 'information'),\n",
       " ('resume', 't'),\n",
       " ('label', 'sequence'),\n",
       " ('sequence', 'l'),\n",
       " ('label', 'sequence', 'l'),\n",
       " ('independent', 'occurrence'),\n",
       " ('same', 'information'),\n",
       " ('unigram', 'model'),\n",
       " ('-', 'gram'),\n",
       " ('tri', '-', 'gram'),\n",
       " ('gram', 'model'),\n",
       " ('-', 'gram', 'model'),\n",
       " ('tri', '-', 'gram', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('general', 'information'),\n",
       " ('part', 'of', 'd'),\n",
       " ('label', 'o'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('kind', 'of', 'educational', 'detailed', 'information'),\n",
       " ('text', 't'),\n",
       " ('t', 'as', 'word'),\n",
       " ('text', 't', 'as', 'word'),\n",
       " ('word', 'sequence'),\n",
       " ('t', 'as', 'word', 'sequence'),\n",
       " ('text', 't', 'as', 'word', 'sequence'),\n",
       " ('sequence', 't'),\n",
       " ('word', 'sequence', 't'),\n",
       " ('t', 'as', 'word', 'sequence', 't'),\n",
       " ('text', 't', 'as', 'word', 'sequence', 't'),\n",
       " ('independent', 'occurrence'),\n",
       " ('same', 'information'),\n",
       " ('chinese', 'resume'),\n",
       " ('resume', 'c='),\n",
       " ('chinese', 'resume', 'c='),\n",
       " ('c=', 'c'),\n",
       " ('resume', 'c=', 'c'),\n",
       " ('chinese', 'resume', 'c=', 'c'),\n",
       " ('c', 'k'),\n",
       " ('chinese', 'word'),\n",
       " ('word', 'segmentation'),\n",
       " ('chinese', 'word', 'segmentation'),\n",
       " ('segmentation', 'system'),\n",
       " ('word', 'segmentation', 'system'),\n",
       " ('chinese', 'word', 'segmentation', 'system'),\n",
       " ('same', 'type'),\n",
       " ('feature', 'set'),\n",
       " ('connected', 'structure'),\n",
       " ('information', 'label'),\n",
       " ('maximum', 'likelihood'),\n",
       " ('likelihood', 'estimation'),\n",
       " ('maximum', 'likelihood', 'estimation'),\n",
       " ('short', 'of', 'training'),\n",
       " ('big', 'problem'),\n",
       " ('unknown', 'word'),\n",
       " ('word', 'w', 'i'),\n",
       " ('unknown', 'word', 'w', 'i'),\n",
       " ('token', '_'),\n",
       " ('-', 'gram'),\n",
       " ('bi', '-', 'gram'),\n",
       " ('sparseness', 'problem'),\n",
       " ('context', 'model'),\n",
       " ('estimation', 'from', 'training'),\n",
       " ('training', 'corpus'),\n",
       " ('estimation', 'from', 'training', 'corpus'),\n",
       " ('back', '-', 'off'),\n",
       " ('off', 'model'),\n",
       " ('back', '-', 'off', 'model'),\n",
       " ('calculated', 'parameter'),\n",
       " ('model', 'with', 'calculated', 'parameter'),\n",
       " ('off', 'model', 'with', 'calculated', 'parameter'),\n",
       " ('back', '-', 'off', 'model', 'with', 'calculated', 'parameter'),\n",
       " ('parameter', 'λ'),\n",
       " ('calculated', 'parameter', 'λ'),\n",
       " ('model', 'with', 'calculated', 'parameter', 'λ'),\n",
       " ('off', 'model', 'with', 'calculated', 'parameter', 'λ'),\n",
       " ('back', '-', 'off', 'model', 'with', 'calculated', 'parameter', 'λ'),\n",
       " ('emission', 'probability'),\n",
       " ('complex', 'model'),\n",
       " ('weighted', 'average'),\n",
       " ('probability', 'p(w'),\n",
       " ('p(w', 'r'),\n",
       " ('probability', 'p(w', 'r'),\n",
       " ('r', '|l'),\n",
       " ('p(w', 'r', '|l'),\n",
       " ('probability', 'p(w', 'r', '|l'),\n",
       " ('word', 'w', 'r'),\n",
       " ('emission', 'probability'),\n",
       " ('p(w', 'r'),\n",
       " ('r', '|l'),\n",
       " ('p(w', 'r', '|l'),\n",
       " ('|l', 'i'),\n",
       " ('r', '|l', 'i'),\n",
       " ('p(w', 'r', '|l', 'i'),\n",
       " ('p(w', 'r'),\n",
       " ('r', '|l'),\n",
       " ('p(w', 'r', '|l'),\n",
       " ('|l', 'i'),\n",
       " ('r', '|l', 'i'),\n",
       " ('p(w', 'r', '|l', 'i'),\n",
       " ('emission', 'probability'),\n",
       " ('total', 'number'),\n",
       " ('unknown', 'word'),\n",
       " ('word', 'w', 'r'),\n",
       " ('unknown', 'word', 'w', 'r'),\n",
       " ('emission', 'probability'),\n",
       " ('state', 'i.'),\n",
       " ('sparseness', 'problem'),\n",
       " ('probability', 'p(l'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('classification', 'problem'),\n",
       " ('classification', 'model'),\n",
       " ('high', 'performance'),\n",
       " ('predefined', 'class'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'p'),\n",
       " ('detailed', 'information', 'p'),\n",
       " ('personal', 'detailed', 'information', 'p'),\n",
       " ('beginning', 'of', 'p'),\n",
       " ('part', 'of', 'p'),\n",
       " ('label', 'o'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('label', 'sequence'),\n",
       " ('sequence', 'as', 'p'),\n",
       " ('label', 'sequence', 'as', 'p'),\n",
       " ('sequence', 'of', 'unit'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('natural', 'sentence'),\n",
       " ('empirical', 'observation'),\n",
       " ('detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('extraction', 'of', 'personal', 'detailed', 'information'),\n",
       " ('text', 't'),\n",
       " ('segmenting', 'method'),\n",
       " ('label', 'sequence'),\n",
       " ('sequence', 'l'),\n",
       " ('label', 'sequence', 'l'),\n",
       " ('key', 'assumption'),\n",
       " ('independence', 'of', 'label'),\n",
       " ('label', 'assignment'),\n",
       " ('independence', 'of', 'label', 'assignment'),\n",
       " ('term', 'in', 'turn'),\n",
       " ('binary', 'classification'),\n",
       " ('classification', 'model'),\n",
       " ('binary', 'classification', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('number', 'of', 'personal', 'detailed', 'information'),\n",
       " ('binary', 'classification'),\n",
       " ('classification', 'task'),\n",
       " ('binary', 'classification', 'task'),\n",
       " ('final', 'decision'),\n",
       " ('weighted', 'voting'),\n",
       " ('good', 'efficiency'),\n",
       " ('efficiency', 'in', 'classification'),\n",
       " ('good', 'efficiency', 'in', 'classification'),\n",
       " ('score', 'in', 'classification'),\n",
       " ('tf×idf', 'as', 'feature'),\n",
       " ('feature', 'weight'),\n",
       " ('tf×idf', 'as', 'feature', 'weight'),\n",
       " ('word', 'frequency'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('detailed', 'information'),\n",
       " ('pass', 'for', 'detailed', 'information'),\n",
       " ('second', 'pass', 'for', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('pass', 'for', 'detailed', 'information', 'extraction'),\n",
       " ('second', 'pass', 'for', 'detailed', 'information', 'extraction'),\n",
       " ('error', 'analysis'),\n",
       " ('general', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('general', 'information', 'extraction'),\n",
       " ('flat', 'model'),\n",
       " ('first', 'pass'),\n",
       " ('fuzzy', 'block'),\n",
       " ('block', 'selection'),\n",
       " ('fuzzy', 'block', 'selection'),\n",
       " ('selection', 'strategy'),\n",
       " ('block', 'selection', 'strategy'),\n",
       " ('fuzzy', 'block', 'selection', 'strategy'),\n",
       " ('general', 'information'),\n",
       " ('target', 'general', 'information'),\n",
       " ('extracting', 'range'),\n",
       " ('hybrid', 'model'),\n",
       " ('cross', 'validation'),\n",
       " ('classifier', 'toolkit'),\n",
       " ('chinese', 'word'),\n",
       " ('word', 'segmentation'),\n",
       " ('chinese', 'word', 'segmentation'),\n",
       " ('entity', 'identification'),\n",
       " ('f', '-', 'score'),\n",
       " ('basic', 'evaluation'),\n",
       " ('special', 'application'),\n",
       " ('application', 'background'),\n",
       " ('special', 'application', 'background'),\n",
       " ('reference', 'instance'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('documentlevel', 'hierarchical', 'structure'),\n",
       " ('structure', 'of', 'resume'),\n",
       " ('hierarchical', 'structure', 'of', 'resume'),\n",
       " ('documentlevel', 'hierarchical', 'structure', 'of', 'resume'),\n",
       " ('sub', '-'),\n",
       " ('-', 'task'),\n",
       " ('sub', '-', 'task'),\n",
       " ('flat', 'model'),\n",
       " ('model', 'vs.', 'flat', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('detailed', 'information'),\n",
       " ('entire', 'resume'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('flat', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('model', 'with', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('model', 'with', 'detailed', 'information', 'extraction'),\n",
       " ('document', '-', 'level'),\n",
       " ('hierarchical', 'structure'),\n",
       " ('level', 'hierarchical', 'structure'),\n",
       " ('document', '-', 'level', 'hierarchical', 'structure'),\n",
       " ('flat', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('loss', 'in', 'recall'),\n",
       " ('average', 'f'),\n",
       " ('f', '-', 'score'),\n",
       " ('average', 'f', '-', 'score'),\n",
       " ('general', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('sub', '-'),\n",
       " ('-', 'task'),\n",
       " ('sub', '-', 'task'),\n",
       " ('detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('extraction', 'range'),\n",
       " ('detailed', 'information'),\n",
       " ('range', 'of', 'detailed', 'information'),\n",
       " ('extraction', 'range', 'of', 'detailed', 'information'),\n",
       " ('general', 'information'),\n",
       " ('result', 'of', 'general', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('general', 'information', 'extraction'),\n",
       " ('result', 'of', 'general', 'information', 'extraction'),\n",
       " ('general', 'information'),\n",
       " ('recall', 'of', 'general', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('recall', 'of', 'detailed', 'information'),\n",
       " ('first', 'pass'),\n",
       " ('hybrid', 'model'),\n",
       " ('second', 'pass'),\n",
       " ('appropriate', 'one'),\n",
       " ('different', 'sub'),\n",
       " ('one', 'for', 'different', 'sub'),\n",
       " ('appropriate', 'one', 'for', 'different', 'sub'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('educational', 'detailed', 'information', 'extraction'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('hybrid', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('independent', 'occurrence'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('occurrence', 'of', 'personal', 'detailed', 'information'),\n",
       " ('independent', 'occurrence', 'of', 'personal', 'detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('error', 'propagation'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('extraction', 'range'),\n",
       " ('detailed', 'information'),\n",
       " ('range', 'of', 'detailed', 'information'),\n",
       " ('extraction', 'range', 'of', 'detailed', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('recall', 'of', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('recall', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('second', 'pass'),\n",
       " ('hybrid', 'model'),\n",
       " ('different', 'sub'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('fuzzy', 'block'),\n",
       " ('block', 'selection'),\n",
       " ('fuzzy', 'block', 'selection'),\n",
       " ('selection', 'strategy'),\n",
       " ('block', 'selection', 'strategy'),\n",
       " ('fuzzy', 'block', 'selection', 'strategy'),\n",
       " ('search', 'scope'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('extraction', 'show'),\n",
       " ('information', 'extraction', 'show'),\n",
       " ('detailed', 'information', 'extraction', 'show'),\n",
       " ('personal', 'detailed', 'information', 'extraction', 'show'),\n",
       " ('exact', 'block'),\n",
       " ('block', 'selection'),\n",
       " ('exact', 'block', 'selection'),\n",
       " ('selection', 'strategy'),\n",
       " ('block', 'selection', 'strategy'),\n",
       " ('exact', 'block', 'selection', 'strategy'),\n",
       " ('fuzzy', 'strategy'),\n",
       " ('average', 'recall'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('recall', 'of', 'personal', 'detailed', 'information'),\n",
       " ('average', 'recall', 'of', 'personal', 'detailed', 'information'),\n",
       " ('average', 'precision'),\n",
       " ('average', 'f'),\n",
       " ('f', '-', 'score'),\n",
       " ('average', 'f', '-', 'score'),\n",
       " ('fuzzy', 'strategy'),\n",
       " ('low', 'accuracy'),\n",
       " ('detailed', 'information'),\n",
       " ('accuracy', 'of', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('accuracy', 'of', 'detailed', 'information', 'extraction'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('f', '-', 'score'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('hybrid', 'model'),\n",
       " ('different', 'pass'),\n",
       " ('contextual', 'structure'),\n",
       " ('structure', 'of', 'information'),\n",
       " ('contextual', 'structure', 'of', 'information'),\n",
       " ('hybrid', 'model'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('average', 'f'),\n",
       " ('good', 'average', 'f'),\n",
       " ('f', '-', 'score'),\n",
       " ('average', 'f', '-', 'score'),\n",
       " ('good', 'average', 'f', '-', 'score'),\n",
       " ('error', 'analysis'),\n",
       " ('certain', 'length'),\n",
       " ('unit', 't'),\n",
       " ('probability', 'p(t'),\n",
       " ('-', 'gram'),\n",
       " ('bi', '-', 'gram'),\n",
       " ('gram', 'model'),\n",
       " ('-', 'gram', 'model'),\n",
       " ('bi', '-', 'gram', 'model'),\n",
       " ('immediate', 'predecessor'),\n",
       " ('current', 'name'),\n",
       " ('name', '-', 'class'),\n",
       " ('current', 'name', '-', 'class'),\n",
       " ('current', 'method'),\n",
       " ('hybrid', 'model'),\n",
       " ('task', 'of', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('task', 'of', 'information', 'extraction'),\n",
       " ('first', 'pass'),\n",
       " ('second', 'pass'),\n",
       " ('general', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('general', 'information', 'extraction'),\n",
       " ('detailed', 'information'),\n",
       " ('educational', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('educational', 'detailed', 'information', 'extraction'),\n",
       " ('strong', 'sequence'),\n",
       " ('sequence', 'of', 'information'),\n",
       " ('strong', 'sequence', 'of', 'information'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('information', 'extraction'),\n",
       " ('detailed', 'information', 'extraction'),\n",
       " ('personal', 'detailed', 'information', 'extraction'),\n",
       " ('detailed', 'information'),\n",
       " ('personal', 'detailed', 'information'),\n",
       " ('sparseness', 'problem')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-restoration",
   "metadata": {},
   "source": [
    "Now, we have to get all of the candidates for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flexible-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e69a81b3e6e4f55ab841a7bfc8bea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "candidates = list(concat(df['text'].progress_apply(get_candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-buffer",
   "metadata": {},
   "source": [
    "### Compute c-values\n",
    "\n",
    "$$\\mbox{C-value}(a)=\\begin{cases}\\log_2|a|\\cdot f(a) & \\mbox{if } a \\mbox{ is not nested}\\\\\\log_2|a|\\left(f(a)-\\frac{1}{P(T_a)}\\sum_{b\\in T_a}f(b)\\right) & \\mbox{otherwise}\\\\\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-thailand",
   "metadata": {},
   "source": [
    "Next, we will count the frequencies of all the candidates and organize them by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loving-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prime-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = defaultdict(Counter)\n",
    "for c in candidates:\n",
    "    freqs[len(c)][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "included-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "utility-dealing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('part', '-', 'of', '-', 'speech'), 213),\n",
       " (('end', '-', 'to', '-', 'end'), 99),\n",
       " (('tree', '-', 'to', '-', 'string'), 57),\n",
       " (('sequence', '-', 'to', '-', 'sequence'), 41),\n",
       " (('state', '-', 'ofthe', '-', 'art'), 37)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[5].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "professional-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suspended-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use 5-1 = 4, to 1, but excluding 1 (use -1)\n",
    "list(range(4, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lonely-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "guided-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', '-', 'of', '-'),\n",
       " ('-', 'of', '-', 'speech'),\n",
       " ('part', '-', 'of'),\n",
       " ('-', 'of', '-'),\n",
       " ('of', '-', 'speech'),\n",
       " ('part', '-'),\n",
       " ('-', 'of'),\n",
       " ('of', '-'),\n",
       " ('-', 'speech')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_subterms(('part', '-', 'of', '-', 'speech')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "widespread-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "nervous-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_value(F, theta):\n",
    "    \n",
    "    termhood = Counter()\n",
    "    longer = defaultdict(list)\n",
    "    \n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)  #This is the extra boost given to longer sequences\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "directed-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dimensional-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  446.00  446 language model\n",
      "  420.27  213 part - of - speech\n",
      "  317.00  458 natural language\n",
      "  310.00  310 training set\n",
      "  307.48  194 sentence - level\n",
      "  298.00  381 machine translation\n",
      "  271.00  271 other hand\n",
      "  265.00  265 test set\n",
      "  261.00  261 previous work\n",
      "  251.00  306 neural network\n",
      "  242.50  153 word - level\n",
      "  238.00  238 word alignment\n",
      "  229.87   99 end - to - end\n",
      "  223.48  141 natural language processing\n",
      "  220.00  220 future work\n",
      "  209.22  132 n - gram\n",
      "  209.22  132 large - scale\n",
      "  198.00  270 co -\n",
      "  194.95  123 f - measure\n",
      "  193.37  122 f - score\n"
     ]
    }
   ],
   "source": [
    "for t, c in terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fluid-personal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84.00   84 target domain\n",
      "   83.00   83 sentence level\n",
      "   82.72   32 part - of - speech tagging\n",
      "   80.83   51 part of speech\n",
      "   80.00   80 first step\n",
      "   80.00   80 head word\n",
      "   80.00   80 sentence pair\n",
      "   79.00   79 time step\n",
      "   78.00   78 baseline system\n",
      "   78.00   78 - word\n",
      "   77.66   49 t - test\n",
      "   77.66   49 character - level\n",
      "   77.00   77 related work\n",
      "   77.00   77 sentence compression\n",
      "   76.00   76 mutual information\n",
      "   76.00   76 re -\n",
      "   76.00   76 small number\n",
      "   76.00   76 deep learning\n",
      "   76.00   76 recent work\n",
      "   76.00   76 text classification\n"
     ]
    }
   ],
   "source": [
    "#Looking at the bottom of the list\n",
    "for t, c in tail(20, terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-amsterdam",
   "metadata": {},
   "source": [
    "Scale this up to all the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-labor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
