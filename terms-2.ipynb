{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assigned-mathematics",
   "metadata": {},
   "source": [
    "# LAB 3: Automated Terminology Extraction\n",
    "\n",
    "Extract technical terms from ACL Anthology\n",
    "\n",
    "Objectives:\n",
    "* part of speech tagging with spacy\n",
    "* extract phrases that match a part of speech pattern\n",
    "* scale processing pipeline with dask\n",
    "* compute c-values\n",
    "\n",
    "## Part I: Test c-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yellow-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sorted-consequence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46189</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46189' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:46189\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinct-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('s3://ling583/acl.parquet', storage_options={'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detailed-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribute along the cluster, so these are the modules needed\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frank-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.from_pandas(df, npartitions=100) #Each task will process 60 articles\n",
    "texts = df['text'].to_bag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-outdoors",
   "metadata": {},
   "source": [
    "### spaCy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ethical-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-marker",
   "metadata": {},
   "source": [
    "Loading a processing pipeline.  This is a small English model.  Will be using part of speech labels only, so we will be excluding modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "individual-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner', 'lemmatizer', 'attribute_ruler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescribed-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "considerable-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN']}},  #JJ = adjective  #NN = noun\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN', 'HYPH']}, 'OP': '*'},  ##IN = preposition ##HYPH = hyphenated speech\n",
    "                      {'TAG': 'NN'}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rocky-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text):\n",
    "    doc = nlp(text)  #tokenize and tag\n",
    "    spans = matcher(doc, as_spans=True)  #find all the tags\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans] #return a list of all spans converted into tuples of normalized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "institutional-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = texts.map(get_candidates) \\\n",
    "             .flatten() \\\n",
    "             .frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "every-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 1.52 s, total: 12.1 s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates = graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fancy-stupid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('polynomial', 'time'), 234),\n",
       " (('recognition', 'phase'), 17),\n",
       " (('input', 'string'), 379),\n",
       " (('spurious', 'ambiguity'), 148),\n",
       " (('function', 'application'), 40),\n",
       " (('relative', 'ordering'), 29),\n",
       " (('considerable', 'interest'), 40),\n",
       " (('large', 'number'), 1357),\n",
       " (('same', 'function'), 26),\n",
       " (('function', 'argument'), 5)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-teacher",
   "metadata": {},
   "source": [
    "### Compute c-values\n",
    "\n",
    "$$\\mbox{C-value}(a)=\\begin{cases}\\log_2|a|\\cdot f(a) & \\mbox{if } a \\mbox{ is not nested}\\\\\\log_2|a|\\left(f(a)-\\frac{1}{P(T_a)}\\sum_{b\\in T_a}f(b)\\right) & \\mbox{otherwise}\\\\\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-effects",
   "metadata": {},
   "source": [
    "Next, we will count the frequencies of all the candidates and organize them by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "working-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "opponent-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = defaultdict(Counter)\n",
    "for c, f in candidates:\n",
    "    freqs[len(c)][c] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sublime-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coordinate-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "upper-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prescribed-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_value(F, theta):\n",
    "    \n",
    "    termhood = Counter()\n",
    "    longer = defaultdict(list)\n",
    "    \n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)  #This is the extra boost given to longer sequences\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "super-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pursuant-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5236.00 5682 language model\n",
      " 4461.67 5060 machine translation\n",
      " 4451.14 2330 part - of - speech\n",
      " 4410.50 5388 natural language\n",
      " 3583.00 3583 training set\n",
      " 3379.00 3920 neural network\n",
      " 3346.00 3346 previous work\n",
      " 3171.75 1366 end - to - end\n",
      " 3012.00 3012 other hand\n",
      " 3003.00 3003 test set\n",
      " 2923.00 2923 future work\n",
      " 2589.83 1634 natural language processing\n",
      " 2370.00 2370 target language\n",
      " 2317.22 1462 sentence - level\n",
      " 2301.37 1452 large - scale\n",
      " 2209.44 1394 word - level\n",
      " 2174.00 2174 parse tree\n",
      " 2144.45 1353 n - gram\n",
      " 2059.00 2059 training corpus\n",
      " 2019.24 1274 f - score\n"
     ]
    }
   ],
   "source": [
    "for t, c in terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "distinct-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  516.70  326 sub - word\n",
      "  515.11  325 chinese word segmentation\n",
      "  515.00  515 lexical information\n",
      "  515.00  515 morphological analysis\n",
      "  515.00  515 input sequence\n",
      "  514.00  514 classification problem\n",
      "  513.00  513 local context\n",
      "  512.00  512 time complexity\n",
      "  511.00  511 text generation\n",
      "  511.00  511 probabilistic model\n",
      "  511.00  511 tree kernel\n",
      "  510.00  510 phrase pair\n",
      "  509.00  509 distributional similarity\n",
      "  508.77  321 natural language generation\n",
      "  508.77  321 f1 - score\n",
      "  508.77  321 hyper - parameter\n",
      "  507.19  320 set of candidate\n",
      "  507.00  507 standard deviation\n",
      "  504.00  504 beam size\n",
      "  503.00  503 dependency relation\n"
     ]
    }
   ],
   "source": [
    "#Looking at the bottom of the list\n",
    "for t, c in tail(20, terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:4d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-administrator",
   "metadata": {},
   "source": [
    "Write this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "iraqi-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terms.txt', 'w') as f:\n",
    "    for term in terms:\n",
    "        print(' '.join(term), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-sunday",
   "metadata": {},
   "source": [
    "This will save a text file of terms into our folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-space",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
